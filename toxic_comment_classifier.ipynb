{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Toxic Comment Classifier"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"d3b04218-0413-4e6c-8751-5d8a404d73a9","_uuid":"0bca9739b82d5d51e1229243e03ea1b6db35c17e"},"source":["## Introduction\n","\n","In this kernel we are going to address the toxic comment classification problem, a multi-label classification problem, via various machine and deep learning techniques.\\\n","We first start by analyzing the data. Then we try to apply techniques such as naive-bayes, logistic regressor, neural network and lstm; we even try a BERT fine tuning.\\\n","The resulting models will then be compared based on their ROC AUC score."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef06cd19-66b6-46bc-bf45-184e12d3f7d4","_uuid":"cca038ca9424a3f66e10262fc9129de807b5f855","collapsed":true,"trusted":true},"outputs":[],"source":["import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","# For evaluation\n","from tqdm import tqdm\n","import transformers\n","import torchmetrics\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer#, CountVectorizer\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score\n","\n","import re, string\n","\n","import torch\n","\n","from transformers import pipeline\n","from tqdm.notebook import tqdm\n","\n","# For LSTM\n","from keras.preprocessing.text import Tokenizer\n","#from keras_preprocessing.sequence import pad_sequences\n","import keras_preprocessing\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D\n","from keras.models import Model\n","from keras import initializers, regularizers, constraints, optimizers, layers"]},{"cell_type":"markdown","metadata":{},"source":["## Load training and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a494f561-0c2f-4a38-8973-6b60c22da357","_uuid":"f70ebe669fcf6b434c595cf6fb7a76120bf7809c","collapsed":true,"trusted":true},"outputs":[],"source":["train = pd.read_csv('../input/train.csv')\n","test = pd.read_csv('../input/test.csv')\n","test_labels = pd.read_csv('../input/test_labels.csv')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Identify the classes"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The comments are labelled as one or more of the following six categories: toxic, severe toxic, obscene, threat, insult and identity hate."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"3996a226-e1ca-4aa8-b39f-6524d4dadb07","_uuid":"2c18461316f17d1d323b1959c8eb4e5448e8a44e"},"source":["## Data analysis\n","\n","The training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train.sample(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"b3b071fb-7a2c-4195-9817-b01983d11c0e","_uuid":"004d2e823056e98afc5adaac433b7afbfe93b82d"},"source":["Here's a couple of examples of comments, one toxic (marked as toxic, obscene, insult), and one with no labels."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9caf5da3-33bb-422d-81c4-fef20fbda1a8","_uuid":"b0d70e9d745411ea6228c95c5f19bd3a2ca6dd55","collapsed":true,"scrolled":true,"trusted":true},"outputs":[],"source":["train['comment_text'][67547]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d57f0b31-c09b-4305-a0b0-0b864e944fd1","_uuid":"1ba9522a65227881a3a55aefaee9de93c4cfd792","collapsed":true,"trusted":true},"outputs":[],"source":["train['comment_text'][156031]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["First of all, let's check if there are any null values in the dataset.\\\n","These will need to be cleaned up eventually later on."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train.isnull().any(),test.isnull().any()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"b8515824-b2dd-4c95-bbf9-dc74c80355db","_uuid":"0151ab55887071aed82d297acb2c6545ed964c2b"},"source":["All rows in the training dataset don't contain null values; specifically, they all contain comments, so there will be no need to clean up null fields.\n","\n","Let's create a summary of the dataset. We also create a 'none' label so we can see how many comments have no labels."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c66f79d1-1d9f-4d94-82c1-8026af198f2a","_uuid":"4ba6ef86c82f073bf411785d971a694348c3efa9","collapsed":true,"trusted":true},"outputs":[],"source":["train['none'] = 1-train[label_cols].max(axis=1)\n","train.describe()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The mean values are very small (some way below 0.05), as 89.8321% of the comments are not labelled in any of the six categories and therefore not considered toxic.\\\n","Let's see the exact numbers for the various categories as well."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('Total rows in train is {}'.format(len(train)))\n","print('Number of unlabelled (positive) comments: {}'.format(train['none'].sum()))\n","print(train[label_cols].sum())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As mentioned, majority of the comments in the training data are not labelled in one or more of these categories.\\\n","Let's look at the character length for the rows in the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd3fe158-4d7f-4b30-ac15-42605240ea4f","_uuid":"9c1a3f81397199fa250a2b642edc7fbc5f9f504e","collapsed":true,"trusted":true},"outputs":[],"source":["lens = train.comment_text.str.len()\n","lens.mean(), lens.std(), lens.max()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The length of the comments varies a lot. Let's look at the histogram plot for text length."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2e55012-4736-425f-84f3-c148ac1f4852","_uuid":"eb68f1c83a5ad11e652ca5f2150993a06d43edb4","collapsed":true,"trusted":true},"outputs":[],"source":["sns.set()\n","lens.hist()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Most of the text length are within 500 characters, with some up to 5,000 characters long.\\\n","Next, let's examine the correlations among the target variables."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = train[label_cols]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["colormap = plt.cm.plasma\n","plt.figure(figsize=(7,7))\n","plt.title('Correlation of features & targets',y=1.05,size=14)\n","sns.heatmap(data.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,\n","           linecolor='white',annot=True);"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Indeed, it looks like some of the labels are higher correlated, e.g. insult-obscene has the highest at 0.74, followed by toxic-obscene and toxic-insult."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data pre-processing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Clean test data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Rows with -1 values in test_labels are not used for evaluation.\n","Therefore, we remove them from test_labels and store their indexes so we can remove them from predictions as well (we need to mantain them in test, otherwise we'll have problems with indixes in predictions)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["indexes = []\n","for index, row in test_labels.iterrows():\n","    if row['toxic'] == -1:\n","        indexes.append(index)\n","test_labels = test_labels.drop(indexes)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Clean up the comment text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n","    text = re.sub('\\W', ' ', text)\n","    text = re.sub('\\s+', ' ', text)\n","    text = text.strip(' ')\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train['comment_text'] = train['comment_text'].map(lambda com : clean_text(com))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test['comment_text'] = test['comment_text'].map(lambda com : clean_text(com))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Vectorize the data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Create a *bag of words* representation, as a *term document matrix*.\\\n","First of all, Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b7f11db7-5c12-4eb8-9f2d-0323d629fed9","_uuid":"b043a3fb66c443fab0129e863c134ec813dadb87","collapsed":true,"trusted":true},"outputs":[],"source":["COMMENT = 'comment_text'\n","re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n","def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Instantiate TfidfVectorizer."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n","              min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n","              smooth_idf=1, sublinear_tf=1, max_features=5000, stop_words='english' )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Learn the vocabulary in the training data, then use it to create a document-term matrix."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trn_term_doc = vec.fit_transform(train[COMMENT])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Transform the test data using the earlier fitted vocabulary, into a document-term matrix."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_term_doc = vec.transform(test[COMMENT])"]},{"cell_type":"markdown","metadata":{"_cell_guid":"4cf3ec26-8237-452b-90c9-831cb0297955","_uuid":"6d215bc460e64d88b08f501d5c5a67c290e40635"},"source":["This creates a *sparse matrix* with only a small number of non-zero elements (*stored elements* in the representation  below)."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c7bdbcc-4451-4477-944c-772e99bac777","_uuid":"8816cc35f66b9fed9c12978fbdef5bb68fae10f4","collapsed":true,"trusted":true},"outputs":[],"source":["trn_term_doc.shape, test_term_doc.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Alt method"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# list_sentences_train = train[\"comment_text\"]\n","# list_sentences_test = test[\"comment_text\"]\n","\n","# max_features = 500 #20000\n","# tokenizer = Tokenizer(num_words=max_features)\n","# tokenizer.fit_on_texts(list(list_sentences_train))\n","# list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n","# list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n","\n","# maxlen = 5000#200\n","# trn_term_doc = pad_sequences(list_tokenized_train, maxlen=maxlen)\n","# test_term_doc = pad_sequences(list_tokenized_test, maxlen=maxlen)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Logistic regression\n","One way to approach a multi-label classification problem is to transform the problem into separate single-class classifier problems. This is known as 'problem transformation'. There are three methods:\n","* _**Binary Relevance.**_ This is probably the simplest which treats each label as a separate single classification problems. The key assumption here though, is that there are no correlation among the various labels.\n","* _**Classifier Chains.**_ In this method, the first classifier is trained on the input X. Then the subsequent classifiers are trained on the input X and all previous classifiers' predictions in the chain. This method attempts to draw the signals from the correlation among preceding target variables.\n","* _**Label Powerset.**_ This method transforms the problem into a multi-class problem  where the multi-class labels are essentially all the unique label combinations. In our case here, where there are six labels, Label Powerset would in effect turn this into a 2^6 or 64-class problem.\n","\n","Next we will try to address the toxic classification problem using the Binary Relevance and the Classifier Chains approaches."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Instantiate the Logistic Regression model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["logreg = LogisticRegression(C=12.0,max_iter=500)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Instantiate matrix to take note of predictions for test data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preds = np.zeros((len(test), len(label_cols)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Binary Relevance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i,label in enumerate(label_cols):\n","    print('... Processing {}'.format(label))\n","    y = train[label]\n","    # train the model using X_dtm & y\n","    logreg.fit(trn_term_doc, y)\n","    # compute the training accuracy\n","    y_pred_X = logreg.predict(trn_term_doc)\n","    print('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n","    # compute the predicted probabilities for X_test_dtm\n","    preds[:,i] = logreg.predict_proba(test_term_doc)[:,1]\n","    #preds[:,i] = logreg.predict_proba(list_tokenized_test)[:,1]"]},{"cell_type":"markdown","metadata":{},"source":["### Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_ids = pd.DataFrame({'id': test[\"id\"]})\n","predictions = pd.concat([test_ids, pd.DataFrame(preds, columns = label_cols)], axis=1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Drop rows not used for evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = predictions.drop(indexes)"]},{"cell_type":"markdown","metadata":{},"source":["Calculating ROC AUC score for each category."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for cat in label_cols:\n","    \n","    print(f\"Category: {cat}\")\n","    print(f\"Sklearn score: {metrics.roc_auc_score(test_labels[cat], predictions[cat], multi_class='ovr')}\")\n","    print(f\"torchmetrics score: {torchmetrics.functional.classification.binary_auroc(torch.tensor(predictions[cat].values),torch.tensor(test_labels[cat].values), thresholds=None)}\")\n","    print(\"#\" * 30)\n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["Calculating mean column-wise ROC AUC score on all categories."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Sklearn score: {metrics.roc_auc_score(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values, predictions[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values, average='macro')}\")\n","print(F\"Torchmetrics score: {torchmetrics.functional.classification.multilabel_auroc(torch.tensor(predictions[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values),torch.tensor(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values),num_labels=6,thresholds=None )}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Classifier Chains"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Create a function to add features."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_feature(X, feature_to_add):\n","    '''\n","    Returns sparse feature matrix with added feature.\n","    feature_to_add can also be a list of features.\n","    '''\n","    from scipy.sparse import csr_matrix, hstack\n","    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Copy trn_term_doc and test_term_doc in train_X and test_X for reusing."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_X, test_X = trn_term_doc, test_term_doc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i,label in enumerate(label_cols):\n","    print('... Processing {}'.format(label))\n","    y = train[label]\n","    # train the model using X_dtm & y\n","    logreg.fit(train_X,y)\n","    # compute the training accuracy\n","    y_pred_X = logreg.predict(train_X)\n","    print('Training Accuracy is {}'.format(accuracy_score(y,y_pred_X)))\n","    # make predictions from test_X\n","    test_y = logreg.predict(test_X)\n","    test_y_prob = logreg.predict_proba(test_X)[:,1]\n","    preds[:,i] = test_y_prob\n","    # chain current label to X_dtm\n","    train_X = add_feature(train_X, y)\n","    print('Shape of X_dtm is now {}'.format(train_X.shape))\n","    # chain current label predictions to test_X_dtm\n","    test_X = add_feature(test_X, test_y)\n","    print('Shape of test_X_dtm is now {}'.format(test_X.shape))"]},{"cell_type":"markdown","metadata":{},"source":["### Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_ids = pd.DataFrame({'id': test[\"id\"]})\n","predictions = pd.concat([test_ids, pd.DataFrame(preds, columns = label_cols)], axis=1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Drop rows not used for evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = predictions.drop(indexes)"]},{"cell_type":"markdown","metadata":{},"source":["Calculating ROC AUC score for each category."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for cat in label_cols:\n","    \n","    print(f\"Category: {cat}\")\n","    print(f\"Sklearn score: {metrics.roc_auc_score(test_labels[cat], predictions[cat], multi_class='ovr')}\")\n","    print(f\"torchmetrics score: {torchmetrics.functional.classification.binary_auroc(torch.tensor(predictions[cat].values),torch.tensor(test_labels[cat].values), thresholds=None)}\")\n","    print(\"#\" * 30)\n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["Calculating mean column-wise ROC AUC score on all categories."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Sklearn score: {metrics.roc_auc_score(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values, predictions[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values, average='macro')}\")\n","print(F\"Torchmetrics score: {torchmetrics.functional.classification.multilabel_auroc(torch.tensor(predictions[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values),torch.tensor(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values),num_labels=6,thresholds=None )}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Naive Bayes - Logistic Regression\n","\n","Here we try using NBSVM (Naive Bayes - Support Vector Machine) but using sklearn's logistic regression rather than SVM, although in practice the two are nearly identical.\\\n","NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classiﬁcation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)."]},{"cell_type":"markdown","metadata":{},"source":["Here's the basic naive bayes feature equation:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def pr(y_i, y):\n","    p = x[y==y_i].sum(0)\n","    return (p+1) / ((y==y_i).sum()+1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = trn_term_doc\n","test_x = test_term_doc"]},{"cell_type":"markdown","metadata":{},"source":["Fit a model for one dependent at a time:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_mdl(y):\n","    y = y.values\n","    r = np.log(pr(1,y) / pr(0,y))\n","    #m = LogisticRegression(C=4, dual=True) # This gives an error\n","    m = LogisticRegression(C=4, dual=False, max_iter=500)\n","    x_nb = x.multiply(r)\n","    return m.fit(x_nb, y), r"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preds = np.zeros((len(test), len(label_cols)))\n","\n","for i, j in enumerate(label_cols):\n","    print('fit', j)\n","    m,r = get_mdl(train[j])\n","    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]"]},{"cell_type":"markdown","metadata":{},"source":["### Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_ids = pd.DataFrame({'id': test[\"id\"]})\n","predictions = pd.concat([test_ids, pd.DataFrame(preds, columns = label_cols)], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["Drop rows not used for evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = predictions.drop(indexes)"]},{"cell_type":"markdown","metadata":{},"source":["Calculating ROC AUC score for each category."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for cat in label_cols:\n","    \n","    print(f\"Category: {cat}\")\n","    print(f\"Sklearn score: {metrics.roc_auc_score(test_labels[cat], predictions[cat], multi_class='ovr')}\")\n","    print(f\"torchmetrics score: {torchmetrics.functional.classification.binary_auroc(torch.tensor(predictions[cat].values),torch.tensor(test_labels[cat].values), thresholds=None)}\")\n","    print(\"#\" * 30)\n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["Calculating mean column-wise ROC AUC score on all categories."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Sklearn score: {metrics.roc_auc_score(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values, predictions[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values, average='macro')}\")\n","print(F\"Torchmetrics score: {torchmetrics.functional.classification.multilabel_auroc(torch.tensor(predictions[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values),torch.tensor(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values),num_labels=6,thresholds=None )}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## LSTM"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The inputs into our networks are our list of encoded sentences. We begin our defining an Input layer that accepts a list of sentences that has a dimension of 200.\\\n","By indicating an empty space after comma, we are telling Keras to infer the number automatically."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n","inp = Input(shape=(5000, )) # maxlen"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embed_size = 128\n","x = Embedding(5000, embed_size)(inp) # max_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = LSTM(60, return_sequences=True,name='lstm_layer')(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = GlobalMaxPool1D()(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# x = Dropout(0.1)(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# x = Dense(50, activation=\"relu\")(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# x = Dropout(0.1)(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = Dense(6, activation=\"sigmoid\")(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = Model(inputs=inp, outputs=x)\n","#model.compile(loss='binary_crossentropy',\n","model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.summary()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Using a batch generator."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def batch_generator(X_data, y_data, batch_size):\n","    samples_per_epoch = X_data.shape[0]\n","    number_of_batches = samples_per_epoch/batch_size\n","    counter=0\n","    #index = np.arange(np.shape(y_data)[0])\n","    shuffle_index = np.arange(np.shape(y_data)[0])\n","    np.random.shuffle(shuffle_index)\n","    while 1:\n","        #index_batch = index[batch_size*counter:batch_size*(counter+1)]\n","        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n","        X_batch = X_data[index_batch,:].todense()\n","        #y_batch = y_data[index_batch]\n","\n","        y_batch = np.zeros((len(index_batch), len(label_cols)), dtype='int64')\n","        for i, label in enumerate(label_cols):\n","            y_batch[:,i] = y_data[label][index_batch]\n","\n","        counter += 1\n","        #yield np.array(X_batch),y_batch\n","        yield np.array(X_batch),np.array(y_batch)\n","        #yield X_batch,np.array(y_batch)\n","        if (counter > number_of_batches):\n","            np.random.shuffle(shuffle_index)\n","            counter=0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = 32\n","epochs = 2\n","#model.fit(trn_term_doc,train[label_cols], batch_size=batch_size, epochs=epochs, validation_split=0.1)\n","model.fit(trn_term_doc.todense(),train[label_cols], batch_size=batch_size, epochs=epochs, validation_split=0.1)\n","#model.fit(batch_generator(trn_term_doc, train[label_cols], batch_size), epochs=epochs)"]},{"cell_type":"markdown","metadata":{},"source":["### Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preds = model.predict(test_term_doc)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_ids = pd.DataFrame({'id': test[\"id\"]})\n","predictions = pd.concat([test_ids, pd.DataFrame(preds, columns = label_cols)], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["Drop rows not used for evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = predictions.drop(indexes)"]},{"cell_type":"markdown","metadata":{},"source":["Calculating ROC AUC score for each category."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for cat in label_cols:\n","    \n","    print(f\"Category: {cat}\")\n","    print(f\"Sklearn score: {metrics.roc_auc_score(test_labels[cat], predictions[cat], multi_class='ovr')}\")\n","    print(f\"torchmetrics score: {torchmetrics.functional.classification.binary_auroc(torch.tensor(predictions[cat].values),torch.tensor(test_labels[cat].values), thresholds=None)}\")\n","    print(\"#\" * 30)\n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["Calculating mean column-wise ROC AUC score on all categories."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Sklearn score: {metrics.roc_auc_score(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values, predictions[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values, average='macro')}\")\n","print(F\"Torchmetrics score: {torchmetrics.functional.classification.multilabel_auroc(torch.tensor(predictions[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values),torch.tensor(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values),num_labels=6,thresholds=None )}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":1}
